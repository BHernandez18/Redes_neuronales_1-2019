{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "domingo 16 de mayo de 2019  \n",
    "  \n",
    "_Benjamín Hernández Cortés_ - _Juan Pablo Rojas Rojas_  \n",
    "_Departamento de Ingeniería Informática (DIINF)_  \n",
    "_Universidad de Santiago de Chile (USACH)_\n",
    "\n",
    "\n",
    "## Laboratorio 3 - Fundamentos de Aprendizaje Profundo con Redes Neuronales\n",
    "___\n",
    "\n",
    "El presente código está orientado hacia la implementación de una red neuronal convolucional o CNN (Convolutional Neural Network), la cual será diseñada y empleada con el fin de clasificar un conjunto de 25.000 imagenes de perros y gatos, que pueden ser descargados a través del siguiente enlace: [Kaggle Cats and Dogs Dataset](https://www.microsoft.com/en-us/download/confirmation.aspx?id=54765).\n",
    "\n",
    "\n",
    "#### Importación de bibliotecas\n",
    "---\n",
    "\n",
    "Las bibliotecas a emplear son:\n",
    "- **os:** Manejo de funcionalidades dependientes del sistema operativo\n",
    "- **numpy:** Herramienta de computación científica, que permite trabajar a través de vectores\n",
    "- **matplotlib:** Visualización gráfica de diversos datos de interés\n",
    "- **tensorflow:** Diseño e implementación de la red neuronal convolucional\n",
    "- **cv2:** Manejo del conjunto de imagenes\n",
    "- **random:** Uso de funciones relacionadas con la aleatoriedad\n",
    "- **tqdm:** Barra de progreso interactiva\n",
    "\n",
    "#### ¡Precauciones!\n",
    "---\n",
    "\n",
    "Para asegurar el correcto funcionamiento de la herramienta, es necesario tener en cuenta las siguientes precauciones y las acciones a tomar, para el reparar algunas funcionalidades que podrían presentar problemas.\n",
    "\n",
    "| <p style='text-align: left;'>**Error**</p>  | <p style='text-align: left;'>**Descripción**</p> |\n",
    "| ------------ | ------------ |\n",
    "| <p style='text-align: justify;'>`UnboundLocalError: local variable 'photoshop' referenced before assignment`</p> |  <p style='text-align: justify;'>Este mensaje de error se presenta al momento de utilizar la clase _ImageDataGenerator_ de _tensorflow.keras.preprocessing.image_. Este error puede ser reparado haciendo una leve modificación al archivo _JpegImagePlugin.py_ asociado a la biblioteca _PIL_, siguiendo los pasos indicados en este [post](https://github.com/python-pillow/Pillow/pull/3771#issuecomment-485104596).</p>|\n",
    "| <p style='text-align: justify;'>`Problema al obtener/cargar las imagenes`</p> |  <p style='text-align: justify;'>El conjunto de imagenes indicado al principio de este trabajo ([Kaggle Cats and Dogs Dataset](https://www.microsoft.com/en-us/download/confirmation.aspx?id=54765)), posee un par de imagenes que no contienen información y por lo tanto, no pueden ser empleados. Las imagenes en cuestión son _/PetImages/Cat/666.jpg_ y _/PetImages/Dog/11702.jpg_. Se recomienda que elimine las imagenes anteriormente indicadas, previo a la ejecución de este archivo.</p>|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "\n",
    "from random import shuffle\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Activation, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17500 images belonging to 2 classes.\n",
      "Found 7498 images belonging to 2 classes.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_11 (Conv2D)           (None, 56, 56, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 18, 18, 32)        0         \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 18, 18, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 10368)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 10369     \n",
      "=================================================================\n",
      "Total params: 11,201\n",
      "Trainable params: 11,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A target array with shape (100, 2) was passed for an output of shape (None, 1) while using as loss `categorical_crossentropy`. This loss expects targets to have the same shape as the output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-75bfb9567511>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m                     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                     validation_data = validation_dataset);\n\u001b[0m",
      "\u001b[0;32m~/anaconda2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1424\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/anaconda2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m       \u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[0;31m# Validate and standardize user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m     x, y, sample_weights = self._standardize_user_data(\n\u001b[0;32m-> 1175\u001b[0;31m         x, y, sample_weight=sample_weight, class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle)\u001b[0m\n\u001b[1;32m   2438\u001b[0m           \u001b[0;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2439\u001b[0m           training_utils.check_loss_and_target_compatibility(\n\u001b[0;32m-> 2440\u001b[0;31m               y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[1;32m   2441\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2442\u001b[0m       \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[0;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[1;32m    510\u001b[0m           raise ValueError('A target array with shape ' + str(y.shape) +\n\u001b[1;32m    511\u001b[0m                            \u001b[0;34m' was passed for an output of shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m                            \u001b[0;34m' while using as loss `'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m                            \u001b[0;34m'This loss expects '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m                            \u001b[0;34m'targets to have the same shape '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: A target array with shape (100, 2) was passed for an output of shape (None, 1) while using as loss `categorical_crossentropy`. This loss expects targets to have the same shape as the output."
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = 60\n",
    "IMAGE_WIDTH, IMAGE_HEIGHT = IMAGE_SIZE, IMAGE_SIZE\n",
    "BATCH_SIZE = 32\n",
    "input_shape = (IMAGE_WIDTH, IMAGE_HEIGHT, 1)\n",
    "DIR = \"C:\\\\Users\\\\Familia Hernández\\\\Desktop\\\\Redes_neuronales_1-2019\\\\Laboratorio 3 - Redes Convolucionales\\\\PetImages\\\\Cat\"\n",
    "\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1.0/255.0,\n",
    "                             validation_split=0.3)\n",
    "\n",
    "# Conjunto de prueba\n",
    "training_dataset = datagen.flow_from_directory('PetImages/',\n",
    "                                               target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                               batch_size=100,\n",
    "                                               color_mode='grayscale',\n",
    "                                               class_mode='categorical',\n",
    "                                               shuffle=True,\n",
    "                                               subset='training')\n",
    "\n",
    "# Conjunto de validación\n",
    "validation_dataset = datagen.flow_from_directory('PetImages/',\n",
    "                                                 target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                                 batch_size=100,\n",
    "                                                 color_mode='grayscale',\n",
    "                                                 class_mode='categorical',\n",
    "                                                 shuffle=True,\n",
    "                                                 subset='validation')\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (5, 5), activation='relu', input_shape=(60,60,1)))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "#model.add(Dense(256, activation='relu'))\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.fit_generator(training_dataset,\n",
    "                    steps_per_epoch=len(training_dataset),\n",
    "                    epochs = 5,\n",
    "                    validation_steps = len(validation_dataset),\n",
    "                    validation_data = validation_dataset);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_train_data():\n",
    "    training_data = []\n",
    "    for img in tqdm(os.listdir(\"PetImages/Cat\")):\n",
    "        label = [0]\n",
    "        path = os.path.join(\"PetImages/Cat\", img)\n",
    "        print(\"Path => \", path)\n",
    "        img = cv2.imread(path,cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, (128,128))\n",
    "        training_data.append([np.array(img),np.array(label)])\n",
    "    shuffle(training_data)\n",
    "    np.save('train_data.npy', training_data)\n",
    "    return training_data\n",
    "\n",
    "a = create_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import JpegImagePlugin\n",
    "JpegImagePlugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
