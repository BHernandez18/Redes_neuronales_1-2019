{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "import itertools as its\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def tanh(x):\n",
    "    return (1.0 - numpy.exp(-2*x))/(1.0 + numpy.exp(-2*x))\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return (1 + tanh(x))*(1 - tanh(x))\n",
    "\n",
    "\n",
    "\n",
    "class TruthTableGenerator():\n",
    "    def generate_table(n_inputs, logic_gate):\n",
    "        table = its.product([0,1], repeat = n_inputs)\n",
    "        table = pd.DataFrame(table)\n",
    "        results = []\n",
    "        \n",
    "        for i in range(n_inputs ** 2):\n",
    "            row = table.loc[[i]].values[0]\n",
    "            if logic_gate == 'AND': results.append( int(all(row)) )\n",
    "            elif logic_gate == 'OR': results.append( int(any(row)) )\n",
    "            elif logic_gate == 'XOR':\n",
    "                tmp_result = row[0] ^ row[1]\n",
    "                for j in range(2, n_inputs):\n",
    "                    tmp_result = tmp_result ^ row[j]\n",
    "                results.append( tmp_result )\n",
    "            else: return None\n",
    "        \n",
    "        table['result'] = results\n",
    "        return table\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, net_arch):\n",
    "        self.activity = tanh\n",
    "        self.activity_derivative = tanh_derivative\n",
    "        self.layers = len(net_arch)\n",
    "        self.steps_per_epoch = 1\n",
    "        self.arch = net_arch\n",
    "        self.weights = []\n",
    "        \n",
    "        for layer in range(self.layers - 1):\n",
    "            w = 2*numpy.random.rand(net_arch[layer] + 1, net_arch[layer+1]) - 1\n",
    "            self.weights.append(w)\n",
    "    \n",
    "    def _forward_prop(self, x):\n",
    "        y = x\n",
    "\n",
    "        for i in range(len(self.weights)-1):\n",
    "            activation = numpy.dot(y[i], self.weights[i])\n",
    "            activity = self.activity(activation)\n",
    "            activity = numpy.concatenate((numpy.ones(1), numpy.array(activity)))\n",
    "            y.append(activity)\n",
    "\n",
    "        # last layer\n",
    "        activation = numpy.dot(y[-1], self.weights[-1])\n",
    "        activity = self.activity(activation)\n",
    "        y.append(activity)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def _back_prop(self, y, target, learning_rate):\n",
    "        error = target - y[-1]\n",
    "        delta_vec = [error * self.activity_derivative(y[-1])]\n",
    "\n",
    "        # we need to begin from the back, from the next to last layer\n",
    "        for i in range(self.layers-2, 0, -1):\n",
    "            error = delta_vec[-1].dot(self.weights[i][1:].T)\n",
    "            error = error*self.activity_derivative(y[i][1:])\n",
    "            delta_vec.append(error)\n",
    "\n",
    "        # Now we need to set the values from back to front\n",
    "        delta_vec.reverse()\n",
    "        \n",
    "        # Finally, we adjust the weights, using the backpropagation rules\n",
    "        for i in range(len(self.weights)):\n",
    "            layer = y[i].reshape(1, self.arch[i]+1)\n",
    "            if i == 1:\n",
    "                print(layer)\n",
    "            delta = delta_vec[i].reshape(1, self.arch[i+1])\n",
    "            self.weights[i] += learning_rate*layer.T.dot(delta)\n",
    "    \n",
    "    #########\n",
    "    # parameters\n",
    "    # ----------\n",
    "    # self:    the class object itself\n",
    "    # data:    the set of all possible pairs of booleans True or False indicated by the integers 1 or 0\n",
    "    # labels:  the result of the logical operation 'xor' on each of those input pairs\n",
    "    #########\n",
    "    def fit(self, data, labels, learning_rate=0.1, epochs=100):\n",
    "        \n",
    "        # Add bias units to the input layer - \n",
    "        # add a \"1\" to the input data (the always-on bias neuron)\n",
    "        ones = numpy.ones((1, data.shape[0]))\n",
    "        Z = numpy.concatenate((ones.T, data), axis=1)\n",
    "        \n",
    "        for k in range(epochs):\n",
    "            if (k+1) % 10000 == 0:\n",
    "                print('epochs: {}'.format(k+1))\n",
    "        \n",
    "            sample = numpy.random.randint(X.shape[0])\n",
    "\n",
    "            # We will now go ahead and set up our feed-forward propagation:\n",
    "            x = [Z[sample]]\n",
    "            y = self._forward_prop(x)\n",
    "\n",
    "            # Now we do our back-propagation of the error to adjust the weights:\n",
    "            target = labels[sample]\n",
    "            self._back_prop(y, target, learning_rate)\n",
    "    \n",
    "    #########\n",
    "    # the predict function is used to check the prediction result of\n",
    "    # this neural network.\n",
    "    # \n",
    "    # parameters\n",
    "    # ----------\n",
    "    # self:   the class object itself\n",
    "    # x:      single input data\n",
    "    #########\n",
    "    def predict_single_data(self, x):\n",
    "        val = numpy.concatenate((numpy.ones(1).T, numpy.array(x)))\n",
    "        for i in range(0, len(self.weights)):\n",
    "            val = self.activity(numpy.dot(val, self.weights[i]))\n",
    "            val = numpy.concatenate((numpy.ones(1).T, numpy.array(val)))\n",
    "        return val[1]\n",
    "    \n",
    "    #########\n",
    "    # the predict function is used to check the prediction result of\n",
    "    # this neural network.\n",
    "    # \n",
    "    # parameters\n",
    "    # ----------\n",
    "    # self:   the class object itself\n",
    "    # X:      the input data array\n",
    "    #########\n",
    "    def predict(self, X):\n",
    "        Y = numpy.array([]).reshape(0, self.arch[-1])\n",
    "        for x in X:\n",
    "            y = numpy.array([[self.predict_single_data(x)]])\n",
    "            Y = numpy.vstack((Y,y))\n",
    "        return Y\n",
    "\n",
    "\n",
    "#numpy.random.seed(0)\n",
    "# Initialize the NeuralNetwork with\n",
    "# 2 input neurons\n",
    "# 2 hidden neurons\n",
    "# 1 output neuron\n",
    "nn = NeuralNetwork([4,3,5,2,1])\n",
    "\n",
    "b = numpy.array(TruthTableGenerator.generate_table(n_inputs=4, logic_gate='XOR'))\n",
    "\n",
    "\n",
    "X = b[:,:-1]\n",
    "y = b[:,-1]\n",
    "\n",
    "\"\"\"\n",
    "# Set the input data\n",
    "X = numpy.array([[0, 0], [0, 1],\n",
    "                [1, 0], [1, 1]])\n",
    "\n",
    "# Set the labels, the correct results for the xor operation\n",
    "y = numpy.array([0, 1, \n",
    "                 1, 0])\n",
    "\"\"\"\n",
    "\n",
    "# Call the fit function and train the network for a chosen number of epochs\n",
    "nn.fit(X, y, epochs=50000)\n",
    "\n",
    "# Show the prediction results\n",
    "print(\"Final prediction\")\n",
    "for s in X:\n",
    "    print(s, nn.predict_single_data(s))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
