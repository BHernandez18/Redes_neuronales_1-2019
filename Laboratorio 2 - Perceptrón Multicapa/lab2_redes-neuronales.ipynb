{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "viernes 03 de mayo de 2019  \n",
    "  \n",
    "_Benjamín Hernández Cortés_ - _Juan Pablo Rojas Rojas_  \n",
    "_Departamento de Ingeniería Informática (DIINF)_  \n",
    "_Universidad de Santiago de Chile (USACH)_\n",
    "\n",
    "\n",
    "## Laboratorio 2 - Fundamentos de Aprendizaje Profundo con Redes Neuronales\n",
    "___\n",
    "\n",
    "El presente código está orientado hacia la implementación de un perceptrón multicapa o red neuronal de múltiples capas (Multi-Layer Neural Network). En primera oportunidad, se utilizará el perceptrón para clasificar y emular compuertas lógicas AND, OR y XOR, tanto de 2 como de 4 entradas. Luego, se realizará una clasificación trabajando con el conjunto de datos [wine](https://archive.ics.uci.edu/ml/datasets/wine) provenientes de la [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php) que contiene 13 atributos en total y que intentan describir clases de vinos provenientes de 3 cultivos distintos.\n",
    "\n",
    "\n",
    "#### Importación de bibliotecas\n",
    "---\n",
    "\n",
    "Las bibliotecas a emplear son:\n",
    "- **Numpy:** Herramienta de computación científica, que nos permitirá trabajar a través de vectores\n",
    "- **Pandas:** Para la manipulación y lectura de datos\n",
    "- **Matplotlib:** Para la visualización gráfica de diversos datos de interés\n",
    "- **Itertools:** Como herramienta para iteración de objetos\n",
    "- **Scikit-learn:** Para la obtención del dataset _wine_ y el uso de herramientas de evaluación de desempeño para los perceptrones multicapa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import itertools as its\n",
    "import sklearn as skl\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definición de la clase TruthTableGenerator\n",
    "---\n",
    "\n",
    "La clase TruthTableGenerator permite generar tablas de verdad para 3 tipos de compuertas lógicas: AND, OR y XOR.\n",
    "\n",
    "En cuanto a las funciones definidas, se tiene:\n",
    "\n",
    "| **Función**  | **Descripción**  |\n",
    "| ------------ | ------------ |\n",
    "| `generate_table(n_inputs, logic_gate)`  |  Genera una tabla de verdad de n-variables (*n_inputs*), basada en una compuerta lógica determinada (*logic_gate*)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TruthTableGenerator():\n",
    "    \n",
    "    def generate_table(n_inputs, logic_gate):\n",
    "        table = its.product([0,1], repeat = n_inputs)\n",
    "        table = pd.DataFrame(table)\n",
    "        results = []\n",
    "        \n",
    "        if logic_gate == 'AND':\n",
    "            for i in range(n_inputs ** 2):\n",
    "                row = table.loc[[i]].values[0]\n",
    "                results.append( int(all(row)) )\n",
    "            \n",
    "        elif logic_gate == 'OR':\n",
    "            for i in range(n_inputs ** 2):\n",
    "                row = table.loc[[i]].values[0]\n",
    "                results.append( int(any(row)) )\n",
    "                    \n",
    "        elif logic_gate == 'XOR':\n",
    "            for i in range(n_inputs ** 2):\n",
    "                row = table.loc[[i]].values[0]\n",
    "                tmp_result = row[0] ^ row[1]\n",
    "                for j in range(2, n_inputs):\n",
    "                    tmp_result = tmp_result ^ row[j]\n",
    "                results.append( tmp_result )\n",
    "        \n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "        table['result'] = results\n",
    "        return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definición de la clase Layer\n",
    "---\n",
    "La clase Layer permite representar una de las capas que conforma al Perceptrón Multicapa. Requiere de 3 parámetros:\n",
    "\n",
    "| <p style='text-align: left;'>**Parámetro**</p> | <p style='text-align: left;'>**Descripción**</p> |\n",
    "| ------------ | ------------ |\n",
    "| `number_of_inputs` |  <p style='text-align: justify;'>Valor númerico entero. Define la cantidad de entradas que recibe la capa, la cual debe ser mayor que 0. Por defecto, se define el número de entradas como 1.</p>|\n",
    "| `number_of_neurons` | <p style='text-align: justify;'>Valor numérico entero. Define la cantidad de neuronas que posee la capa, la cual debe ser mayor que 0. Por defecto, se define el número de neuronas como 1.</p> |\n",
    "| `activation_function` | <p style='text-align: justify;'>Cadena de caracteres. Define la función de activación que empleará la capa, la cual puede ser sigmoide (_sigmoid_) o ReLU (_relu_).</p> |\n",
    "\n",
    "En cuanto a las funciones definidas, se tiene:\n",
    "\n",
    "| <p style='text-align: left;'>**Función**</p>  | <p style='text-align: left;'>**Descripción**</p> |\n",
    "| ------------ | ------------ |\n",
    "| `calculate_output(input_vector)` |  <p style='text-align: justify;'>Realiza una predicción para un conjunto de datos (*input_vector*), empleando los valores actuales de los pesos (*weights*) asociados a la capa.</p>|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
      "0      14.23        1.71  2.43               15.6      127.0           2.80   \n",
      "1      13.20        1.78  2.14               11.2      100.0           2.65   \n",
      "2      13.16        2.36  2.67               18.6      101.0           2.80   \n",
      "3      14.37        1.95  2.50               16.8      113.0           3.85   \n",
      "4      13.24        2.59  2.87               21.0      118.0           2.80   \n",
      "5      14.20        1.76  2.45               15.2      112.0           3.27   \n",
      "6      14.39        1.87  2.45               14.6       96.0           2.50   \n",
      "7      14.06        2.15  2.61               17.6      121.0           2.60   \n",
      "8      14.83        1.64  2.17               14.0       97.0           2.80   \n",
      "9      13.86        1.35  2.27               16.0       98.0           2.98   \n",
      "10     14.10        2.16  2.30               18.0      105.0           2.95   \n",
      "11     14.12        1.48  2.32               16.8       95.0           2.20   \n",
      "12     13.75        1.73  2.41               16.0       89.0           2.60   \n",
      "13     14.75        1.73  2.39               11.4       91.0           3.10   \n",
      "14     14.38        1.87  2.38               12.0      102.0           3.30   \n",
      "15     13.63        1.81  2.70               17.2      112.0           2.85   \n",
      "16     14.30        1.92  2.72               20.0      120.0           2.80   \n",
      "17     13.83        1.57  2.62               20.0      115.0           2.95   \n",
      "18     14.19        1.59  2.48               16.5      108.0           3.30   \n",
      "19     13.64        3.10  2.56               15.2      116.0           2.70   \n",
      "20     14.06        1.63  2.28               16.0      126.0           3.00   \n",
      "21     12.93        3.80  2.65               18.6      102.0           2.41   \n",
      "22     13.71        1.86  2.36               16.6      101.0           2.61   \n",
      "23     12.85        1.60  2.52               17.8       95.0           2.48   \n",
      "24     13.50        1.81  2.61               20.0       96.0           2.53   \n",
      "25     13.05        2.05  3.22               25.0      124.0           2.63   \n",
      "26     13.39        1.77  2.62               16.1       93.0           2.85   \n",
      "27     13.30        1.72  2.14               17.0       94.0           2.40   \n",
      "28     13.87        1.90  2.80               19.4      107.0           2.95   \n",
      "29     14.02        1.68  2.21               16.0       96.0           2.65   \n",
      "..       ...         ...   ...                ...        ...            ...   \n",
      "148    13.32        3.24  2.38               21.5       92.0           1.93   \n",
      "149    13.08        3.90  2.36               21.5      113.0           1.41   \n",
      "150    13.50        3.12  2.62               24.0      123.0           1.40   \n",
      "151    12.79        2.67  2.48               22.0      112.0           1.48   \n",
      "152    13.11        1.90  2.75               25.5      116.0           2.20   \n",
      "153    13.23        3.30  2.28               18.5       98.0           1.80   \n",
      "154    12.58        1.29  2.10               20.0      103.0           1.48   \n",
      "155    13.17        5.19  2.32               22.0       93.0           1.74   \n",
      "156    13.84        4.12  2.38               19.5       89.0           1.80   \n",
      "157    12.45        3.03  2.64               27.0       97.0           1.90   \n",
      "158    14.34        1.68  2.70               25.0       98.0           2.80   \n",
      "159    13.48        1.67  2.64               22.5       89.0           2.60   \n",
      "160    12.36        3.83  2.38               21.0       88.0           2.30   \n",
      "161    13.69        3.26  2.54               20.0      107.0           1.83   \n",
      "162    12.85        3.27  2.58               22.0      106.0           1.65   \n",
      "163    12.96        3.45  2.35               18.5      106.0           1.39   \n",
      "164    13.78        2.76  2.30               22.0       90.0           1.35   \n",
      "165    13.73        4.36  2.26               22.5       88.0           1.28   \n",
      "166    13.45        3.70  2.60               23.0      111.0           1.70   \n",
      "167    12.82        3.37  2.30               19.5       88.0           1.48   \n",
      "168    13.58        2.58  2.69               24.5      105.0           1.55   \n",
      "169    13.40        4.60  2.86               25.0      112.0           1.98   \n",
      "170    12.20        3.03  2.32               19.0       96.0           1.25   \n",
      "171    12.77        2.39  2.28               19.5       86.0           1.39   \n",
      "172    14.16        2.51  2.48               20.0       91.0           1.68   \n",
      "173    13.71        5.65  2.45               20.5       95.0           1.68   \n",
      "174    13.40        3.91  2.48               23.0      102.0           1.80   \n",
      "175    13.27        4.28  2.26               20.0      120.0           1.59   \n",
      "176    13.17        2.59  2.37               20.0      120.0           1.65   \n",
      "177    14.13        4.10  2.74               24.5       96.0           2.05   \n",
      "\n",
      "     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
      "0          3.06                  0.28             2.29         5.640000  1.04   \n",
      "1          2.76                  0.26             1.28         4.380000  1.05   \n",
      "2          3.24                  0.30             2.81         5.680000  1.03   \n",
      "3          3.49                  0.24             2.18         7.800000  0.86   \n",
      "4          2.69                  0.39             1.82         4.320000  1.04   \n",
      "5          3.39                  0.34             1.97         6.750000  1.05   \n",
      "6          2.52                  0.30             1.98         5.250000  1.02   \n",
      "7          2.51                  0.31             1.25         5.050000  1.06   \n",
      "8          2.98                  0.29             1.98         5.200000  1.08   \n",
      "9          3.15                  0.22             1.85         7.220000  1.01   \n",
      "10         3.32                  0.22             2.38         5.750000  1.25   \n",
      "11         2.43                  0.26             1.57         5.000000  1.17   \n",
      "12         2.76                  0.29             1.81         5.600000  1.15   \n",
      "13         3.69                  0.43             2.81         5.400000  1.25   \n",
      "14         3.64                  0.29             2.96         7.500000  1.20   \n",
      "15         2.91                  0.30             1.46         7.300000  1.28   \n",
      "16         3.14                  0.33             1.97         6.200000  1.07   \n",
      "17         3.40                  0.40             1.72         6.600000  1.13   \n",
      "18         3.93                  0.32             1.86         8.700000  1.23   \n",
      "19         3.03                  0.17             1.66         5.100000  0.96   \n",
      "20         3.17                  0.24             2.10         5.650000  1.09   \n",
      "21         2.41                  0.25             1.98         4.500000  1.03   \n",
      "22         2.88                  0.27             1.69         3.800000  1.11   \n",
      "23         2.37                  0.26             1.46         3.930000  1.09   \n",
      "24         2.61                  0.28             1.66         3.520000  1.12   \n",
      "25         2.68                  0.47             1.92         3.580000  1.13   \n",
      "26         2.94                  0.34             1.45         4.800000  0.92   \n",
      "27         2.19                  0.27             1.35         3.950000  1.02   \n",
      "28         2.97                  0.37             1.76         4.500000  1.25   \n",
      "29         2.33                  0.26             1.98         4.700000  1.04   \n",
      "..          ...                   ...              ...              ...   ...   \n",
      "148        0.76                  0.45             1.25         8.420000  0.55   \n",
      "149        1.39                  0.34             1.14         9.400000  0.57   \n",
      "150        1.57                  0.22             1.25         8.600000  0.59   \n",
      "151        1.36                  0.24             1.26        10.800000  0.48   \n",
      "152        1.28                  0.26             1.56         7.100000  0.61   \n",
      "153        0.83                  0.61             1.87        10.520000  0.56   \n",
      "154        0.58                  0.53             1.40         7.600000  0.58   \n",
      "155        0.63                  0.61             1.55         7.900000  0.60   \n",
      "156        0.83                  0.48             1.56         9.010000  0.57   \n",
      "157        0.58                  0.63             1.14         7.500000  0.67   \n",
      "158        1.31                  0.53             2.70        13.000000  0.57   \n",
      "159        1.10                  0.52             2.29        11.750000  0.57   \n",
      "160        0.92                  0.50             1.04         7.650000  0.56   \n",
      "161        0.56                  0.50             0.80         5.880000  0.96   \n",
      "162        0.60                  0.60             0.96         5.580000  0.87   \n",
      "163        0.70                  0.40             0.94         5.280000  0.68   \n",
      "164        0.68                  0.41             1.03         9.580000  0.70   \n",
      "165        0.47                  0.52             1.15         6.620000  0.78   \n",
      "166        0.92                  0.43             1.46        10.680000  0.85   \n",
      "167        0.66                  0.40             0.97        10.260000  0.72   \n",
      "168        0.84                  0.39             1.54         8.660000  0.74   \n",
      "169        0.96                  0.27             1.11         8.500000  0.67   \n",
      "170        0.49                  0.40             0.73         5.500000  0.66   \n",
      "171        0.51                  0.48             0.64         9.899999  0.57   \n",
      "172        0.70                  0.44             1.24         9.700000  0.62   \n",
      "173        0.61                  0.52             1.06         7.700000  0.64   \n",
      "174        0.75                  0.43             1.41         7.300000  0.70   \n",
      "175        0.69                  0.43             1.35        10.200000  0.59   \n",
      "176        0.68                  0.53             1.46         9.300000  0.60   \n",
      "177        0.76                  0.56             1.35         9.200000  0.61   \n",
      "\n",
      "     od280/od315_of_diluted_wines  proline  target  \n",
      "0                            3.92   1065.0       0  \n",
      "1                            3.40   1050.0       0  \n",
      "2                            3.17   1185.0       0  \n",
      "3                            3.45   1480.0       0  \n",
      "4                            2.93    735.0       0  \n",
      "5                            2.85   1450.0       0  \n",
      "6                            3.58   1290.0       0  \n",
      "7                            3.58   1295.0       0  \n",
      "8                            2.85   1045.0       0  \n",
      "9                            3.55   1045.0       0  \n",
      "10                           3.17   1510.0       0  \n",
      "11                           2.82   1280.0       0  \n",
      "12                           2.90   1320.0       0  \n",
      "13                           2.73   1150.0       0  \n",
      "14                           3.00   1547.0       0  \n",
      "15                           2.88   1310.0       0  \n",
      "16                           2.65   1280.0       0  \n",
      "17                           2.57   1130.0       0  \n",
      "18                           2.82   1680.0       0  \n",
      "19                           3.36    845.0       0  \n",
      "20                           3.71    780.0       0  \n",
      "21                           3.52    770.0       0  \n",
      "22                           4.00   1035.0       0  \n",
      "23                           3.63   1015.0       0  \n",
      "24                           3.82    845.0       0  \n",
      "25                           3.20    830.0       0  \n",
      "26                           3.22   1195.0       0  \n",
      "27                           2.77   1285.0       0  \n",
      "28                           3.40    915.0       0  \n",
      "29                           3.59   1035.0       0  \n",
      "..                            ...      ...     ...  \n",
      "148                          1.62    650.0       2  \n",
      "149                          1.33    550.0       2  \n",
      "150                          1.30    500.0       2  \n",
      "151                          1.47    480.0       2  \n",
      "152                          1.33    425.0       2  \n",
      "153                          1.51    675.0       2  \n",
      "154                          1.55    640.0       2  \n",
      "155                          1.48    725.0       2  \n",
      "156                          1.64    480.0       2  \n",
      "157                          1.73    880.0       2  \n",
      "158                          1.96    660.0       2  \n",
      "159                          1.78    620.0       2  \n",
      "160                          1.58    520.0       2  \n",
      "161                          1.82    680.0       2  \n",
      "162                          2.11    570.0       2  \n",
      "163                          1.75    675.0       2  \n",
      "164                          1.68    615.0       2  \n",
      "165                          1.75    520.0       2  \n",
      "166                          1.56    695.0       2  \n",
      "167                          1.75    685.0       2  \n",
      "168                          1.80    750.0       2  \n",
      "169                          1.92    630.0       2  \n",
      "170                          1.83    510.0       2  \n",
      "171                          1.63    470.0       2  \n",
      "172                          1.71    660.0       2  \n",
      "173                          1.74    740.0       2  \n",
      "174                          1.56    750.0       2  \n",
      "175                          1.56    835.0       2  \n",
      "176                          1.62    840.0       2  \n",
      "177                          1.60    560.0       2  \n",
      "\n",
      "[178 rows x 14 columns]\n",
      "##############333\n",
      "      alcohol  malic_acid       ash  alcalinity_of_ash  magnesium  \\\n",
      "0    0.842105    0.191700  0.572193           0.257732   0.619565   \n",
      "1    0.571053    0.205534  0.417112           0.030928   0.326087   \n",
      "2    0.560526    0.320158  0.700535           0.412371   0.336957   \n",
      "3    0.878947    0.239130  0.609626           0.319588   0.467391   \n",
      "4    0.581579    0.365613  0.807487           0.536082   0.521739   \n",
      "5    0.834211    0.201581  0.582888           0.237113   0.456522   \n",
      "6    0.884211    0.223320  0.582888           0.206186   0.282609   \n",
      "7    0.797368    0.278656  0.668449           0.360825   0.554348   \n",
      "8    1.000000    0.177866  0.433155           0.175258   0.293478   \n",
      "9    0.744737    0.120553  0.486631           0.278351   0.304348   \n",
      "10   0.807895    0.280632  0.502674           0.381443   0.380435   \n",
      "11   0.813158    0.146245  0.513369           0.319588   0.271739   \n",
      "12   0.715789    0.195652  0.561497           0.278351   0.206522   \n",
      "13   0.978947    0.195652  0.550802           0.041237   0.228261   \n",
      "14   0.881579    0.223320  0.545455           0.072165   0.347826   \n",
      "15   0.684211    0.211462  0.716578           0.340206   0.456522   \n",
      "16   0.860526    0.233202  0.727273           0.484536   0.543478   \n",
      "17   0.736842    0.164032  0.673797           0.484536   0.489130   \n",
      "18   0.831579    0.167984  0.598930           0.304124   0.413043   \n",
      "19   0.686842    0.466403  0.641711           0.237113   0.500000   \n",
      "20   0.797368    0.175889  0.491979           0.278351   0.608696   \n",
      "21   0.500000    0.604743  0.689840           0.412371   0.347826   \n",
      "22   0.705263    0.221344  0.534759           0.309278   0.336957   \n",
      "23   0.478947    0.169960  0.620321           0.371134   0.271739   \n",
      "24   0.650000    0.211462  0.668449           0.484536   0.282609   \n",
      "25   0.531579    0.258893  0.994652           0.742268   0.586957   \n",
      "26   0.621053    0.203557  0.673797           0.283505   0.250000   \n",
      "27   0.597368    0.193676  0.417112           0.329897   0.260870   \n",
      "28   0.747368    0.229249  0.770053           0.453608   0.402174   \n",
      "29   0.786842    0.185771  0.454545           0.278351   0.282609   \n",
      "..        ...         ...       ...                ...        ...   \n",
      "148  0.602632    0.494071  0.545455           0.561856   0.239130   \n",
      "149  0.539474    0.624506  0.534759           0.561856   0.467391   \n",
      "150  0.650000    0.470356  0.673797           0.690722   0.576087   \n",
      "151  0.463158    0.381423  0.598930           0.587629   0.456522   \n",
      "152  0.547368    0.229249  0.743316           0.768041   0.500000   \n",
      "153  0.578947    0.505929  0.491979           0.407216   0.304348   \n",
      "154  0.407895    0.108696  0.395722           0.484536   0.358696   \n",
      "155  0.563158    0.879447  0.513369           0.587629   0.250000   \n",
      "156  0.739474    0.667984  0.545455           0.458763   0.206522   \n",
      "157  0.373684    0.452569  0.684492           0.845361   0.293478   \n",
      "158  0.871053    0.185771  0.716578           0.742268   0.304348   \n",
      "159  0.644737    0.183794  0.684492           0.613402   0.206522   \n",
      "160  0.350000    0.610672  0.545455           0.536082   0.195652   \n",
      "161  0.700000    0.498024  0.631016           0.484536   0.402174   \n",
      "162  0.478947    0.500000  0.652406           0.587629   0.391304   \n",
      "163  0.507895    0.535573  0.529412           0.407216   0.391304   \n",
      "164  0.723684    0.399209  0.502674           0.587629   0.217391   \n",
      "165  0.710526    0.715415  0.481283           0.613402   0.195652   \n",
      "166  0.636842    0.584980  0.663102           0.639175   0.445652   \n",
      "167  0.471053    0.519763  0.502674           0.458763   0.195652   \n",
      "168  0.671053    0.363636  0.711230           0.716495   0.380435   \n",
      "169  0.623684    0.762846  0.802139           0.742268   0.456522   \n",
      "170  0.307895    0.452569  0.513369           0.432990   0.282609   \n",
      "171  0.457895    0.326087  0.491979           0.458763   0.173913   \n",
      "172  0.823684    0.349802  0.598930           0.484536   0.228261   \n",
      "173  0.705263    0.970356  0.582888           0.510309   0.271739   \n",
      "174  0.623684    0.626482  0.598930           0.639175   0.347826   \n",
      "175  0.589474    0.699605  0.481283           0.484536   0.543478   \n",
      "176  0.563158    0.365613  0.540107           0.484536   0.543478   \n",
      "177  0.815789    0.664032  0.737968           0.716495   0.282609   \n",
      "\n",
      "     total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\n",
      "0         0.627586    0.573840              0.283019         0.593060   \n",
      "1         0.575862    0.510549              0.245283         0.274448   \n",
      "2         0.627586    0.611814              0.320755         0.757098   \n",
      "3         0.989655    0.664557              0.207547         0.558360   \n",
      "4         0.627586    0.495781              0.490566         0.444795   \n",
      "5         0.789655    0.643460              0.396226         0.492114   \n",
      "6         0.524138    0.459916              0.320755         0.495268   \n",
      "7         0.558621    0.457806              0.339623         0.264984   \n",
      "8         0.627586    0.556962              0.301887         0.495268   \n",
      "9         0.689655    0.592827              0.169811         0.454259   \n",
      "10        0.679310    0.628692              0.169811         0.621451   \n",
      "11        0.420690    0.440928              0.245283         0.365931   \n",
      "12        0.558621    0.510549              0.301887         0.441640   \n",
      "13        0.731034    0.706751              0.566038         0.757098   \n",
      "14        0.800000    0.696203              0.301887         0.804416   \n",
      "15        0.644828    0.542194              0.320755         0.331230   \n",
      "16        0.627586    0.590717              0.377358         0.492114   \n",
      "17        0.679310    0.645570              0.509434         0.413249   \n",
      "18        0.800000    0.757384              0.358491         0.457413   \n",
      "19        0.593103    0.567511              0.075472         0.394322   \n",
      "20        0.696552    0.597046              0.207547         0.533123   \n",
      "21        0.493103    0.436709              0.226415         0.495268   \n",
      "22        0.562069    0.535865              0.264151         0.403785   \n",
      "23        0.517241    0.428270              0.245283         0.331230   \n",
      "24        0.534483    0.478903              0.283019         0.394322   \n",
      "25        0.568966    0.493671              0.641509         0.476341   \n",
      "26        0.644828    0.548523              0.396226         0.328076   \n",
      "27        0.489655    0.390295              0.264151         0.296530   \n",
      "28        0.679310    0.554852              0.452830         0.425868   \n",
      "29        0.575862    0.419831              0.245283         0.495268   \n",
      "..             ...         ...                   ...              ...   \n",
      "148       0.327586    0.088608              0.603774         0.264984   \n",
      "149       0.148276    0.221519              0.396226         0.230284   \n",
      "150       0.144828    0.259494              0.169811         0.264984   \n",
      "151       0.172414    0.215190              0.207547         0.268139   \n",
      "152       0.420690    0.198312              0.245283         0.362776   \n",
      "153       0.282759    0.103376              0.905660         0.460568   \n",
      "154       0.172414    0.050633              0.754717         0.312303   \n",
      "155       0.262069    0.061181              0.905660         0.359621   \n",
      "156       0.282759    0.103376              0.660377         0.362776   \n",
      "157       0.317241    0.050633              0.943396         0.230284   \n",
      "158       0.627586    0.204641              0.754717         0.722397   \n",
      "159       0.558621    0.160338              0.735849         0.593060   \n",
      "160       0.455172    0.122363              0.698113         0.198738   \n",
      "161       0.293103    0.046414              0.698113         0.123028   \n",
      "162       0.231034    0.054852              0.886792         0.173502   \n",
      "163       0.141379    0.075949              0.509434         0.167192   \n",
      "164       0.127586    0.071730              0.528302         0.195584   \n",
      "165       0.103448    0.027426              0.735849         0.233438   \n",
      "166       0.248276    0.122363              0.566038         0.331230   \n",
      "167       0.172414    0.067511              0.509434         0.176656   \n",
      "168       0.196552    0.105485              0.490566         0.356467   \n",
      "169       0.344828    0.130802              0.264151         0.220820   \n",
      "170       0.093103    0.031646              0.509434         0.100946   \n",
      "171       0.141379    0.035865              0.660377         0.072555   \n",
      "172       0.241379    0.075949              0.584906         0.261830   \n",
      "173       0.241379    0.056962              0.735849         0.205047   \n",
      "174       0.282759    0.086498              0.566038         0.315457   \n",
      "175       0.210345    0.073840              0.566038         0.296530   \n",
      "176       0.231034    0.071730              0.754717         0.331230   \n",
      "177       0.368966    0.088608              0.811321         0.296530   \n",
      "\n",
      "     color_intensity       hue  od280/od315_of_diluted_wines   proline  target  \n",
      "0           0.372014  0.455285                      0.970696  0.561341       0  \n",
      "1           0.264505  0.463415                      0.780220  0.550642       0  \n",
      "2           0.375427  0.447154                      0.695971  0.646933       0  \n",
      "3           0.556314  0.308943                      0.798535  0.857347       0  \n",
      "4           0.259386  0.455285                      0.608059  0.325963       0  \n",
      "5           0.466724  0.463415                      0.578755  0.835949       0  \n",
      "6           0.338737  0.439024                      0.846154  0.721826       0  \n",
      "7           0.321672  0.471545                      0.846154  0.725392       0  \n",
      "8           0.334471  0.487805                      0.578755  0.547076       0  \n",
      "9           0.506826  0.430894                      0.835165  0.547076       0  \n",
      "10          0.381399  0.626016                      0.695971  0.878745       0  \n",
      "11          0.317406  0.560976                      0.567766  0.714693       0  \n",
      "12          0.368601  0.544715                      0.597070  0.743224       0  \n",
      "13          0.351536  0.626016                      0.534799  0.621969       0  \n",
      "14          0.530717  0.585366                      0.633700  0.905136       0  \n",
      "15          0.513652  0.650407                      0.589744  0.736091       0  \n",
      "16          0.419795  0.479675                      0.505495  0.714693       0  \n",
      "17          0.453925  0.528455                      0.476190  0.607703       0  \n",
      "18          0.633106  0.609756                      0.567766  1.000000       0  \n",
      "19          0.325939  0.390244                      0.765568  0.404422       0  \n",
      "20          0.372867  0.495935                      0.893773  0.358060       0  \n",
      "21          0.274744  0.447154                      0.824176  0.350927       0  \n",
      "22          0.215017  0.512195                      1.000000  0.539943       0  \n",
      "23          0.226109  0.495935                      0.864469  0.525678       0  \n",
      "24          0.191126  0.520325                      0.934066  0.404422       0  \n",
      "25          0.196246  0.528455                      0.706960  0.393723       0  \n",
      "26          0.300341  0.357724                      0.714286  0.654066       0  \n",
      "27          0.227816  0.439024                      0.549451  0.718260       0  \n",
      "28          0.274744  0.626016                      0.780220  0.454351       0  \n",
      "29          0.291809  0.455285                      0.849817  0.539943       0  \n",
      "..               ...       ...                           ...       ...     ...  \n",
      "148         0.609215  0.056911                      0.128205  0.265335       2  \n",
      "149         0.692833  0.073171                      0.021978  0.194009       2  \n",
      "150         0.624573  0.089431                      0.010989  0.158345       2  \n",
      "151         0.812287  0.000000                      0.073260  0.144080       2  \n",
      "152         0.496587  0.105691                      0.021978  0.104850       2  \n",
      "153         0.788396  0.065041                      0.087912  0.283167       2  \n",
      "154         0.539249  0.081301                      0.102564  0.258203       2  \n",
      "155         0.564846  0.097561                      0.076923  0.318830       2  \n",
      "156         0.659556  0.073171                      0.135531  0.144080       2  \n",
      "157         0.530717  0.154472                      0.168498  0.429387       2  \n",
      "158         1.000000  0.073171                      0.252747  0.272468       2  \n",
      "159         0.893345  0.073171                      0.186813  0.243937       2  \n",
      "160         0.543515  0.065041                      0.113553  0.172611       2  \n",
      "161         0.392491  0.390244                      0.201465  0.286733       2  \n",
      "162         0.366894  0.317073                      0.307692  0.208274       2  \n",
      "163         0.341297  0.162602                      0.175824  0.283167       2  \n",
      "164         0.708191  0.178862                      0.150183  0.240371       2  \n",
      "165         0.455631  0.243902                      0.175824  0.172611       2  \n",
      "166         0.802048  0.300813                      0.106227  0.297432       2  \n",
      "167         0.766212  0.195122                      0.175824  0.290300       2  \n",
      "168         0.629693  0.211382                      0.194139  0.336662       2  \n",
      "169         0.616041  0.154472                      0.238095  0.251070       2  \n",
      "170         0.360068  0.146341                      0.205128  0.165478       2  \n",
      "171         0.735495  0.073171                      0.131868  0.136947       2  \n",
      "172         0.718430  0.113821                      0.161172  0.272468       2  \n",
      "173         0.547782  0.130081                      0.172161  0.329529       2  \n",
      "174         0.513652  0.178862                      0.106227  0.336662       2  \n",
      "175         0.761092  0.089431                      0.106227  0.397290       2  \n",
      "176         0.684300  0.097561                      0.128205  0.400856       2  \n",
      "177         0.675768  0.105691                      0.120879  0.201141       2  \n",
      "\n",
      "[178 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "class Layer:\n",
    "    def __init__(self,\n",
    "                 number_of_inputs = 1,\n",
    "                 number_of_neurons = 1,\n",
    "                 activation_function = 'sigmoid'):\n",
    "        self.number_of_neurons = number_of_neurons\n",
    "        self.number_of_inputs = number_of_inputs\n",
    "        self.activation_function = activation_function\n",
    "        self.weights = np.random.rand(self.number_of_inputs + 1, self.number_of_neurons)\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        \n",
    "    def calculate_output(self, input_vector):\n",
    "        self.input = np.append(input_vector, 1)\n",
    "        sum_ = np.matmul(self.input, self.weights)\n",
    "        if self.activation_function == 'sigmoid': self.output = sigmoid(sum_)\n",
    "        elif self.activation_function == 'relu': self.output = relu(sum_)\n",
    "        else: self.output = None\n",
    "        return self.output\n",
    "        \n",
    "            \n",
    "\n",
    "class MLP:\n",
    "    def __init__(self,\n",
    "                 data = None,\n",
    "                 number_of_inputs = 1,\n",
    "                 number_of_hidden_layers = 0,\n",
    "                 number_of_neurons_for_each_layer = None,\n",
    "                 activation_functions_for_each_layer = None):\n",
    "        \n",
    "        self.data = data\n",
    "        self.data_nrows, self.data_ncols = data.shape\n",
    "        self.number_of_hidden_layers = number_of_hidden_layers\n",
    "        self.number_of_neurons_for_each_layer = number_of_neurons_for_each_layer\n",
    "        self.activation_functions_for_each_layer = activation_functions_for_each_layer\n",
    "        self.layers = []\n",
    "        \n",
    "        # Primero se crea una capa escondida (hidden layer) según la cantidad de entradas\n",
    "        # definidas para el perceptrón. Si no se define un número de capas escondidas,\n",
    "        # entonces solamente se creará la capa de salida (output layer), recreando a un\n",
    "        # perceptrón simple.\n",
    "        \n",
    "        self.layers.append(\n",
    "            Layer(number_of_inputs = number_of_inputs,\n",
    "                  number_of_neurons = number_of_neurons_for_each_layer[0],\n",
    "                  activation_function = activation_functions_for_each_layer[0])\n",
    "        )\n",
    "        for i in range(1, self.number_of_hidden_layers + 1):\n",
    "            self.layers.append(\n",
    "                Layer(number_of_inputs = number_of_neurons_for_each_layer[i-1],\n",
    "                      number_of_neurons = number_of_neurons_for_each_layer[i],\n",
    "                      activation_function = activation_functions_for_each_layer[i])\n",
    "            )\n",
    "    \n",
    "    def training(self, number_of_iterations):\n",
    "        errors_series = []\n",
    "        for i in range(number_of_iterations):\n",
    "            errors = 0\n",
    "            for j in range(self.data_nrows):\n",
    "                row = np.array(self.data.loc[[j]].values[0])\n",
    "                predicted_value = self.predict( row[:-1], row[-1] )\n",
    "                #editado\n",
    "                error = row[-1] - predicted_value[-1]\n",
    "                errors += int(error != 0.0)\n",
    "            errors_series.append(errors)\n",
    "        return np.array(errors_series)\n",
    "    \n",
    "    def predict(self, input_vector, target_vector):\n",
    "        output_vector = None\n",
    "        for layer in self.layers:\n",
    "            output_vector = layer.calculate_output(input_vector)\n",
    "            input_vector = output_vector\n",
    "        self.adjust_weights(target_vector)\n",
    "        return output_vector\n",
    "    \n",
    "    def predict_value(self, input_vector):\n",
    "        output_vector = None\n",
    "        for layer in self.layers:\n",
    "            output_vector = layer.calculate_output(input_vector)\n",
    "            input_vector = output_vector\n",
    "        return output_vector\n",
    "    \n",
    "    \n",
    "    def adjust_weights(self, target_vector):\n",
    "        flag = True\n",
    "        errors = []\n",
    "        deltas_values = []\n",
    "        deltas_bias_values = []\n",
    "        \n",
    "        # Calculo del error para la capa de salida\n",
    "        error = np.subtract(target_vector, self.layers[-1].output)\n",
    "        \n",
    "        delta_out = error * sigmoid_derivative(self.layers[-1].output)\n",
    "        \n",
    "        gradiente_out = delta_out*self.layers[-1].input\n",
    "    \n",
    "        weights_before = self.layers[-1].weights\n",
    "    \n",
    "        \n",
    "        for index in range(len(self.layers[-1].weights)):\n",
    "            self.layers[-1].weights[index] -= 0.5*gradiente_out[index]\n",
    "            \n",
    "        \n",
    "        for i in list(reversed(range(len(self.layers))))[1:]:\n",
    "            \n",
    "            error_hidden = delta_out*weights_before[:-1]\n",
    "            \n",
    "            #bien\n",
    "            \n",
    "            v_derivate = sigmoid_derivative(self.layers[i].output)\n",
    "            \n",
    "            delta_hidden = error_hidden\n",
    "        \n",
    "            \n",
    "            for n in range(len(error_hidden)):\n",
    "                delta_hidden[n][0] *= v_derivate[n]\n",
    "                \n",
    "            #bien\n",
    "            \n",
    "            delta_out = delta_hidden\n",
    "            ###################################\n",
    "            \n",
    "            \n",
    "            gradiente_hidden = delta_hidden * self.layers[i].input\n",
    "        \n",
    "                 \n",
    "            \n",
    "            \n",
    "            weights_before = self.layers[i].weights\n",
    "            #columna de weights resta con fila de gradientes\n",
    "            \n",
    "            for j in range(len(self.layers[i].weights)):\n",
    "                for k in range(len(gradiente_hidden)):\n",
    "                    self.layers[i].weights[j][k] -= 0.5*gradiente_hidden[k][j]\n",
    "            #bien\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 -  sigmoid(x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "                \n",
    "def sklearn_to_df(sklearn_dataset):\n",
    "    df = pd.DataFrame(sklearn_dataset.data, columns=sklearn_dataset.feature_names)\n",
    "    df['target'] = pd.Series(sklearn_dataset.target)\n",
    "    return df\n",
    "\n",
    "def preprocesamiento(dataset):\n",
    "    data_col_names = dataset.columns.values\n",
    "    for name in data_col_names[:-1]:\n",
    "        a[name] = preprocessing.scale(a[name])\n",
    "        a[name] = ((a[name] - a[name].min())/(a[name].max()-a[name].min()))\n",
    "        \n",
    "    return dataset\n",
    "\n",
    "a = sklearn_to_df(load_wine())\n",
    "\n",
    "b = TruthTableGenerator.generate_table(n_inputs=4, logic_gate='XOR')\n",
    "perceptron = MLP(data = b,\n",
    "                 number_of_inputs=4,\n",
    "                 number_of_hidden_layers=1,\n",
    "                 number_of_neurons_for_each_layer=[2,1],\n",
    "                 activation_functions_for_each_layer=['sigmoid', 'sigmoid'])\n",
    "\n",
    "nrows, ncols = a.shape\n",
    "print(a)\n",
    "print(\"##############333\")\n",
    "print(preprocesamiento(a))\n",
    "#predicted_value = self.predict( row[:-1], row[-1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
