{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "viernes 03 de mayo de 2019  \n",
    "  \n",
    "_Benjamín Hernández Cortés_ - _Juan Pablo Rojas Rojas_  \n",
    "_Departamento de Ingeniería Informática (DIINF)_  \n",
    "_Universidad de Santiago de Chile (USACH)_\n",
    "\n",
    "\n",
    "## Laboratorio 2 - Fundamentos de Aprendizaje Profundo con Redes Neuronales\n",
    "___\n",
    "\n",
    "El presente código está orientado hacia la implementación de un perceptrón multicapa o red neuronal de múltiples capas (Multi-Layer Neural Network). En primera oportunidad, se utilizará el perceptrón para clasificar y emular compuertas lógicas AND, OR y XOR, tanto de 2 como de 4 entradas. Luego, se realizará una clasificación trabajando con el conjunto de datos [wine](https://archive.ics.uci.edu/ml/datasets/wine) provenientes de la [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php) que contiene 13 atributos en total y que intentan describir clases de vinos provenientes de 3 cultivos distintos.\n",
    "\n",
    "\n",
    "#### Importación de bibliotecas\n",
    "---\n",
    "\n",
    "Las bibliotecas a emplear son:\n",
    "- **Numpy:** Herramienta de computación científica, que nos permitirá trabajar a través de vectores\n",
    "- **Pandas:** Para la manipulación y lectura de datos\n",
    "- **Matplotlib:** Para la visualización gráfica de diversos datos de interés\n",
    "- **Itertools:** Como herramienta para iteración de objetos\n",
    "- **Scikit-learn:** Para la obtención del dataset _wine_ y el uso de herramientas de evaluación de desempeño para los perceptrones multicapa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import itertools as its\n",
    "import sklearn as skl\n",
    "from sklearn.datasets import load_wine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definición de la clase TruthTableGenerator\n",
    "---\n",
    "\n",
    "La clase TruthTableGenerator permite generar tablas de verdad para 3 tipos de compuertas lógicas: AND, OR y XOR.\n",
    "\n",
    "En cuanto a las funciones definidas, se tiene:\n",
    "\n",
    "| **Función**  | **Descripción**  |\n",
    "| ------------ | ------------ |\n",
    "| `generate_table(n_inputs, logic_gate)`  |  Genera una tabla de verdad de n-variables (*n_inputs*), basada en una compuerta lógica determinada (*logic_gate*)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TruthTableGenerator():\n",
    "    \n",
    "    def generate_table(n_inputs, logic_gate):\n",
    "        table = its.product([0,1], repeat = n_inputs)\n",
    "        table = pd.DataFrame(table)\n",
    "        results = []\n",
    "        \n",
    "        if logic_gate == 'AND':\n",
    "            for i in range(n_inputs ** 2):\n",
    "                row = table.loc[[i]].values[0]\n",
    "                results.append( int(all(row)) )\n",
    "            \n",
    "        elif logic_gate == 'OR':\n",
    "            for i in range(n_inputs ** 2):\n",
    "                row = table.loc[[i]].values[0]\n",
    "                results.append( int(any(row)) )\n",
    "                    \n",
    "        elif logic_gate == 'XOR':\n",
    "            for i in range(n_inputs ** 2):\n",
    "                row = table.loc[[i]].values[0]\n",
    "                tmp_result = row[0] ^ row[1]\n",
    "                for j in range(2, n_inputs):\n",
    "                    tmp_result = tmp_result ^ row[j]\n",
    "                results.append( tmp_result )\n",
    "        \n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "        table['result'] = results\n",
    "        return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definición de la clase Layer\n",
    "---\n",
    "La clase Layer permite representar una de las capas que conforma al Perceptrón Multicapa. Requiere de 3 parámetros:\n",
    "\n",
    "| <p style='text-align: left;'>**Parámetro**</p> | <p style='text-align: left;'>**Descripción**</p> |\n",
    "| ------------ | ------------ |\n",
    "| `number_of_inputs` |  <p style='text-align: justify;'>Valor númerico entero. Define la cantidad de entradas que recibe la capa, la cual debe ser mayor que 0. Por defecto, se define el número de entradas como 1.</p>|\n",
    "| `number_of_neurons` | <p style='text-align: justify;'>Valor numérico entero. Define la cantidad de neuronas que posee la capa, la cual debe ser mayor que 0. Por defecto, se define el número de neuronas como 1.</p> |\n",
    "| `activation_function` | <p style='text-align: justify;'>Cadena de caracteres. Define la función de activación que empleará la capa, la cual puede ser sigmoide (_sigmoid_) o ReLU (_relu_).</p> |\n",
    "\n",
    "En cuanto a las funciones definidas, se tiene:\n",
    "\n",
    "| <p style='text-align: left;'>**Función**</p>  | <p style='text-align: left;'>**Descripción**</p> |\n",
    "| ------------ | ------------ |\n",
    "| `calculate_output(input_vector)` |  <p style='text-align: justify;'>Realiza una predicción para un conjunto de datos (*input_vector*), empleando los valores actuales de los pesos (*weights*) asociados a la capa.</p>|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
      "0      14.23        1.71  2.43               15.6      127.0           2.80   \n",
      "1      13.20        1.78  2.14               11.2      100.0           2.65   \n",
      "2      13.16        2.36  2.67               18.6      101.0           2.80   \n",
      "3      14.37        1.95  2.50               16.8      113.0           3.85   \n",
      "4      13.24        2.59  2.87               21.0      118.0           2.80   \n",
      "5      14.20        1.76  2.45               15.2      112.0           3.27   \n",
      "6      14.39        1.87  2.45               14.6       96.0           2.50   \n",
      "7      14.06        2.15  2.61               17.6      121.0           2.60   \n",
      "8      14.83        1.64  2.17               14.0       97.0           2.80   \n",
      "9      13.86        1.35  2.27               16.0       98.0           2.98   \n",
      "10     14.10        2.16  2.30               18.0      105.0           2.95   \n",
      "11     14.12        1.48  2.32               16.8       95.0           2.20   \n",
      "12     13.75        1.73  2.41               16.0       89.0           2.60   \n",
      "13     14.75        1.73  2.39               11.4       91.0           3.10   \n",
      "14     14.38        1.87  2.38               12.0      102.0           3.30   \n",
      "15     13.63        1.81  2.70               17.2      112.0           2.85   \n",
      "16     14.30        1.92  2.72               20.0      120.0           2.80   \n",
      "17     13.83        1.57  2.62               20.0      115.0           2.95   \n",
      "18     14.19        1.59  2.48               16.5      108.0           3.30   \n",
      "19     13.64        3.10  2.56               15.2      116.0           2.70   \n",
      "20     14.06        1.63  2.28               16.0      126.0           3.00   \n",
      "21     12.93        3.80  2.65               18.6      102.0           2.41   \n",
      "22     13.71        1.86  2.36               16.6      101.0           2.61   \n",
      "23     12.85        1.60  2.52               17.8       95.0           2.48   \n",
      "24     13.50        1.81  2.61               20.0       96.0           2.53   \n",
      "25     13.05        2.05  3.22               25.0      124.0           2.63   \n",
      "26     13.39        1.77  2.62               16.1       93.0           2.85   \n",
      "27     13.30        1.72  2.14               17.0       94.0           2.40   \n",
      "28     13.87        1.90  2.80               19.4      107.0           2.95   \n",
      "29     14.02        1.68  2.21               16.0       96.0           2.65   \n",
      "..       ...         ...   ...                ...        ...            ...   \n",
      "148    13.32        3.24  2.38               21.5       92.0           1.93   \n",
      "149    13.08        3.90  2.36               21.5      113.0           1.41   \n",
      "150    13.50        3.12  2.62               24.0      123.0           1.40   \n",
      "151    12.79        2.67  2.48               22.0      112.0           1.48   \n",
      "152    13.11        1.90  2.75               25.5      116.0           2.20   \n",
      "153    13.23        3.30  2.28               18.5       98.0           1.80   \n",
      "154    12.58        1.29  2.10               20.0      103.0           1.48   \n",
      "155    13.17        5.19  2.32               22.0       93.0           1.74   \n",
      "156    13.84        4.12  2.38               19.5       89.0           1.80   \n",
      "157    12.45        3.03  2.64               27.0       97.0           1.90   \n",
      "158    14.34        1.68  2.70               25.0       98.0           2.80   \n",
      "159    13.48        1.67  2.64               22.5       89.0           2.60   \n",
      "160    12.36        3.83  2.38               21.0       88.0           2.30   \n",
      "161    13.69        3.26  2.54               20.0      107.0           1.83   \n",
      "162    12.85        3.27  2.58               22.0      106.0           1.65   \n",
      "163    12.96        3.45  2.35               18.5      106.0           1.39   \n",
      "164    13.78        2.76  2.30               22.0       90.0           1.35   \n",
      "165    13.73        4.36  2.26               22.5       88.0           1.28   \n",
      "166    13.45        3.70  2.60               23.0      111.0           1.70   \n",
      "167    12.82        3.37  2.30               19.5       88.0           1.48   \n",
      "168    13.58        2.58  2.69               24.5      105.0           1.55   \n",
      "169    13.40        4.60  2.86               25.0      112.0           1.98   \n",
      "170    12.20        3.03  2.32               19.0       96.0           1.25   \n",
      "171    12.77        2.39  2.28               19.5       86.0           1.39   \n",
      "172    14.16        2.51  2.48               20.0       91.0           1.68   \n",
      "173    13.71        5.65  2.45               20.5       95.0           1.68   \n",
      "174    13.40        3.91  2.48               23.0      102.0           1.80   \n",
      "175    13.27        4.28  2.26               20.0      120.0           1.59   \n",
      "176    13.17        2.59  2.37               20.0      120.0           1.65   \n",
      "177    14.13        4.10  2.74               24.5       96.0           2.05   \n",
      "\n",
      "     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
      "0          3.06                  0.28             2.29         5.640000  1.04   \n",
      "1          2.76                  0.26             1.28         4.380000  1.05   \n",
      "2          3.24                  0.30             2.81         5.680000  1.03   \n",
      "3          3.49                  0.24             2.18         7.800000  0.86   \n",
      "4          2.69                  0.39             1.82         4.320000  1.04   \n",
      "5          3.39                  0.34             1.97         6.750000  1.05   \n",
      "6          2.52                  0.30             1.98         5.250000  1.02   \n",
      "7          2.51                  0.31             1.25         5.050000  1.06   \n",
      "8          2.98                  0.29             1.98         5.200000  1.08   \n",
      "9          3.15                  0.22             1.85         7.220000  1.01   \n",
      "10         3.32                  0.22             2.38         5.750000  1.25   \n",
      "11         2.43                  0.26             1.57         5.000000  1.17   \n",
      "12         2.76                  0.29             1.81         5.600000  1.15   \n",
      "13         3.69                  0.43             2.81         5.400000  1.25   \n",
      "14         3.64                  0.29             2.96         7.500000  1.20   \n",
      "15         2.91                  0.30             1.46         7.300000  1.28   \n",
      "16         3.14                  0.33             1.97         6.200000  1.07   \n",
      "17         3.40                  0.40             1.72         6.600000  1.13   \n",
      "18         3.93                  0.32             1.86         8.700000  1.23   \n",
      "19         3.03                  0.17             1.66         5.100000  0.96   \n",
      "20         3.17                  0.24             2.10         5.650000  1.09   \n",
      "21         2.41                  0.25             1.98         4.500000  1.03   \n",
      "22         2.88                  0.27             1.69         3.800000  1.11   \n",
      "23         2.37                  0.26             1.46         3.930000  1.09   \n",
      "24         2.61                  0.28             1.66         3.520000  1.12   \n",
      "25         2.68                  0.47             1.92         3.580000  1.13   \n",
      "26         2.94                  0.34             1.45         4.800000  0.92   \n",
      "27         2.19                  0.27             1.35         3.950000  1.02   \n",
      "28         2.97                  0.37             1.76         4.500000  1.25   \n",
      "29         2.33                  0.26             1.98         4.700000  1.04   \n",
      "..          ...                   ...              ...              ...   ...   \n",
      "148        0.76                  0.45             1.25         8.420000  0.55   \n",
      "149        1.39                  0.34             1.14         9.400000  0.57   \n",
      "150        1.57                  0.22             1.25         8.600000  0.59   \n",
      "151        1.36                  0.24             1.26        10.800000  0.48   \n",
      "152        1.28                  0.26             1.56         7.100000  0.61   \n",
      "153        0.83                  0.61             1.87        10.520000  0.56   \n",
      "154        0.58                  0.53             1.40         7.600000  0.58   \n",
      "155        0.63                  0.61             1.55         7.900000  0.60   \n",
      "156        0.83                  0.48             1.56         9.010000  0.57   \n",
      "157        0.58                  0.63             1.14         7.500000  0.67   \n",
      "158        1.31                  0.53             2.70        13.000000  0.57   \n",
      "159        1.10                  0.52             2.29        11.750000  0.57   \n",
      "160        0.92                  0.50             1.04         7.650000  0.56   \n",
      "161        0.56                  0.50             0.80         5.880000  0.96   \n",
      "162        0.60                  0.60             0.96         5.580000  0.87   \n",
      "163        0.70                  0.40             0.94         5.280000  0.68   \n",
      "164        0.68                  0.41             1.03         9.580000  0.70   \n",
      "165        0.47                  0.52             1.15         6.620000  0.78   \n",
      "166        0.92                  0.43             1.46        10.680000  0.85   \n",
      "167        0.66                  0.40             0.97        10.260000  0.72   \n",
      "168        0.84                  0.39             1.54         8.660000  0.74   \n",
      "169        0.96                  0.27             1.11         8.500000  0.67   \n",
      "170        0.49                  0.40             0.73         5.500000  0.66   \n",
      "171        0.51                  0.48             0.64         9.899999  0.57   \n",
      "172        0.70                  0.44             1.24         9.700000  0.62   \n",
      "173        0.61                  0.52             1.06         7.700000  0.64   \n",
      "174        0.75                  0.43             1.41         7.300000  0.70   \n",
      "175        0.69                  0.43             1.35        10.200000  0.59   \n",
      "176        0.68                  0.53             1.46         9.300000  0.60   \n",
      "177        0.76                  0.56             1.35         9.200000  0.61   \n",
      "\n",
      "     od280/od315_of_diluted_wines  proline  target  \n",
      "0                            3.92   1065.0       0  \n",
      "1                            3.40   1050.0       0  \n",
      "2                            3.17   1185.0       0  \n",
      "3                            3.45   1480.0       0  \n",
      "4                            2.93    735.0       0  \n",
      "5                            2.85   1450.0       0  \n",
      "6                            3.58   1290.0       0  \n",
      "7                            3.58   1295.0       0  \n",
      "8                            2.85   1045.0       0  \n",
      "9                            3.55   1045.0       0  \n",
      "10                           3.17   1510.0       0  \n",
      "11                           2.82   1280.0       0  \n",
      "12                           2.90   1320.0       0  \n",
      "13                           2.73   1150.0       0  \n",
      "14                           3.00   1547.0       0  \n",
      "15                           2.88   1310.0       0  \n",
      "16                           2.65   1280.0       0  \n",
      "17                           2.57   1130.0       0  \n",
      "18                           2.82   1680.0       0  \n",
      "19                           3.36    845.0       0  \n",
      "20                           3.71    780.0       0  \n",
      "21                           3.52    770.0       0  \n",
      "22                           4.00   1035.0       0  \n",
      "23                           3.63   1015.0       0  \n",
      "24                           3.82    845.0       0  \n",
      "25                           3.20    830.0       0  \n",
      "26                           3.22   1195.0       0  \n",
      "27                           2.77   1285.0       0  \n",
      "28                           3.40    915.0       0  \n",
      "29                           3.59   1035.0       0  \n",
      "..                            ...      ...     ...  \n",
      "148                          1.62    650.0       2  \n",
      "149                          1.33    550.0       2  \n",
      "150                          1.30    500.0       2  \n",
      "151                          1.47    480.0       2  \n",
      "152                          1.33    425.0       2  \n",
      "153                          1.51    675.0       2  \n",
      "154                          1.55    640.0       2  \n",
      "155                          1.48    725.0       2  \n",
      "156                          1.64    480.0       2  \n",
      "157                          1.73    880.0       2  \n",
      "158                          1.96    660.0       2  \n",
      "159                          1.78    620.0       2  \n",
      "160                          1.58    520.0       2  \n",
      "161                          1.82    680.0       2  \n",
      "162                          2.11    570.0       2  \n",
      "163                          1.75    675.0       2  \n",
      "164                          1.68    615.0       2  \n",
      "165                          1.75    520.0       2  \n",
      "166                          1.56    695.0       2  \n",
      "167                          1.75    685.0       2  \n",
      "168                          1.80    750.0       2  \n",
      "169                          1.92    630.0       2  \n",
      "170                          1.83    510.0       2  \n",
      "171                          1.63    470.0       2  \n",
      "172                          1.71    660.0       2  \n",
      "173                          1.74    740.0       2  \n",
      "174                          1.56    750.0       2  \n",
      "175                          1.56    835.0       2  \n",
      "176                          1.62    840.0       2  \n",
      "177                          1.60    560.0       2  \n",
      "\n",
      "[178 rows x 14 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 13,  8,  8,  8,  8,  8,\n",
       "        8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "        8,  8,  8,  8,  8,  8])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Layer:\n",
    "    def __init__(self,\n",
    "                 number_of_inputs = 1,\n",
    "                 number_of_neurons = 1,\n",
    "                 activation_function = 'sigmoid'):\n",
    "        self.number_of_neurons = number_of_neurons\n",
    "        self.number_of_inputs = number_of_inputs\n",
    "        self.activation_function = activation_function\n",
    "        self.weights = np.random.rand(self.number_of_inputs + 1, self.number_of_neurons)\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        \n",
    "    def calculate_output(self, input_vector):\n",
    "        self.input = np.append(input_vector, 1)\n",
    "        sum_ = np.matmul(self.input, self.weights)\n",
    "        if self.activation_function == 'sigmoid': self.output = sigmoid(sum_)\n",
    "        elif self.activation_function == 'relu': self.output = relu(sum_)\n",
    "        else: self.output = None\n",
    "        return self.output\n",
    "        \n",
    "            \n",
    "\n",
    "class MLP:\n",
    "    def __init__(self,\n",
    "                 data = None,\n",
    "                 number_of_inputs = 1,\n",
    "                 number_of_hidden_layers = 0,\n",
    "                 number_of_neurons_for_each_layer = None,\n",
    "                 activation_functions_for_each_layer = None):\n",
    "        \n",
    "        self.data = data\n",
    "        self.data_nrows, self.data_ncols = data.shape\n",
    "        self.number_of_hidden_layers = number_of_hidden_layers\n",
    "        self.number_of_neurons_for_each_layer = number_of_neurons_for_each_layer\n",
    "        self.activation_functions_for_each_layer = activation_functions_for_each_layer\n",
    "        self.layers = []\n",
    "        \n",
    "        # Primero se crea una capa escondida (hidden layer) según la cantidad de entradas\n",
    "        # definidas para el perceptrón. Si no se define un número de capas escondidas,\n",
    "        # entonces solamente se creará la capa de salida (output layer), recreando a un\n",
    "        # perceptrón simple.\n",
    "        \n",
    "        self.layers.append(\n",
    "            Layer(number_of_inputs = number_of_inputs,\n",
    "                  number_of_neurons = number_of_neurons_for_each_layer[0],\n",
    "                  activation_function = activation_functions_for_each_layer[0])\n",
    "        )\n",
    "        for i in range(1, self.number_of_hidden_layers + 1):\n",
    "            self.layers.append(\n",
    "                Layer(number_of_inputs = number_of_neurons_for_each_layer[i-1],\n",
    "                      number_of_neurons = number_of_neurons_for_each_layer[i],\n",
    "                      activation_function = activation_functions_for_each_layer[i])\n",
    "            )\n",
    "    \n",
    "    def training(self, number_of_iterations):\n",
    "        errors_series = []\n",
    "        for i in range(number_of_iterations):\n",
    "            errors = 0\n",
    "            for j in range(self.data_nrows):\n",
    "                row = np.array(self.data.loc[[j]].values[0])\n",
    "                predicted_value = self.predict( row[:-1], row[-1] )\n",
    "                #editado\n",
    "                error = row[-1] - predicted_value[-1]\n",
    "                errors += int(error != 0.0)\n",
    "            errors_series.append(errors)\n",
    "        return np.array(errors_series)\n",
    "    \n",
    "    def predict(self, input_vector, target_vector):\n",
    "        output_vector = None\n",
    "        for layer in self.layers:\n",
    "            output_vector = layer.calculate_output(input_vector)\n",
    "            input_vector = output_vector\n",
    "        self.adjust_weights(target_vector)\n",
    "        return output_vector\n",
    "    \n",
    "    def predict_value(self, input_vector):\n",
    "        output_vector = None\n",
    "        for layer in self.layers:\n",
    "            output_vector = layer.calculate_output(input_vector)\n",
    "            input_vector = output_vector\n",
    "        return output_vector\n",
    "    \n",
    "    \n",
    "    def adjust_weights(self, target_vector):\n",
    "        flag = True\n",
    "        errors = []\n",
    "        deltas_values = []\n",
    "        deltas_bias_values = []\n",
    "        \n",
    "        # Calculo del error para la capa de salida\n",
    "        error = np.subtract(target_vector, self.layers[-1].output)\n",
    "        \n",
    "        delta_out = error * sigmoid_derivative(self.layers[-1].output)\n",
    "        \n",
    "        gradiente_out = delta_out*self.layers[-1].input\n",
    "    \n",
    "        weights_before = self.layers[-1].weights\n",
    "    \n",
    "        \n",
    "        for index in range(len(self.layers[-1].weights)):\n",
    "            self.layers[-1].weights[index] -= 0.5*gradiente_out[index]\n",
    "            \n",
    "        \n",
    "        for i in list(reversed(range(len(self.layers))))[1:]:\n",
    "            \n",
    "            error_hidden = delta_out*weights_before[:-1]\n",
    "            \n",
    "            #bien\n",
    "            \n",
    "            v_derivate = sigmoid_derivative(self.layers[i].output)\n",
    "            \n",
    "            delta_hidden = error_hidden\n",
    "        \n",
    "            \n",
    "            for n in range(len(error_hidden)):\n",
    "                delta_hidden[n][0] *= v_derivate[n]\n",
    "                \n",
    "            #bien\n",
    "            \n",
    "            delta_out = delta_hidden\n",
    "            ###################################\n",
    "            \n",
    "            \n",
    "            gradiente_hidden = delta_hidden * self.layers[i].input\n",
    "        \n",
    "                 \n",
    "            \n",
    "            \n",
    "            weights_before = self.layers[i].weights\n",
    "            #columna de weights resta con fila de gradientes\n",
    "            \n",
    "            for j in range(len(self.layers[i].weights)):\n",
    "                for k in range(len(gradiente_hidden)):\n",
    "                    self.layers[i].weights[j][k] -= 0.5*gradiente_hidden[k][j]\n",
    "            #bien\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 -  sigmoid(x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "                \n",
    "def sklearn_to_df(sklearn_dataset):\n",
    "    df = pd.DataFrame(sklearn_dataset.data, columns=sklearn_dataset.feature_names)\n",
    "    df['target'] = pd.Series(sklearn_dataset.target)\n",
    "    return df\n",
    "\n",
    "    \n",
    "\n",
    "a = sklearn_to_df(load_wine())\n",
    "\n",
    "b = TruthTableGenerator.generate_table(n_inputs=4, logic_gate='XOR')\n",
    "print(a)\n",
    "perceptron = MLP(data = b,\n",
    "                 number_of_inputs=4,\n",
    "                 number_of_hidden_layers=1,\n",
    "                 number_of_neurons_for_each_layer=[3,1],\n",
    "                 activation_functions_for_each_layer=['sigmoid', 'sigmoid'])\n",
    "\n",
    "perceptron.training(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
